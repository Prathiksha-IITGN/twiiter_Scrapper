# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t1e3sDZ6SGf8zwAH5Do7JxRD7dYmysTa
"""

#!pip install pymongo

import pymongo
from pymongo import MongoClient

#!pip install streamlit
import streamlit as st

#!pip install snscrape

import streamlit as st
import pymongo
import pandas as pd
from pymongo import MongoClient
import snscrape.modules.twitter as sntwitter
import base64

client = MongoClient("mongodb://localhost:27017/")
db = client["twitter_data"]
collection = db["tweets"]

st.title("Twitter Scraper")

# Define scrape_twitter_data function
def scrape_twitter_data(hashtag, start_date, end_date, tweet_count):
    tweets_list = []
    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#' + hashtag + ' since:' + start_date + ' until:' + end_date).get_items()):
        if i > tweet_count:
            break
        tweets_list.append(
            {
                "date": tweet.date,
                "id": tweet.id,
                "content": tweet.rawContent,
                "username": tweet.user.username,
                "url": tweet.url,
                "source": tweet.source
            }
        )
    return tweets_list

# Get hashtag, start date, end date, and tweet count from user
hashtag = st.text_input("Enter a hashtag:")
start_date = st.text_input("Enter start date (YYYY-MM-DD):")
end_date = st.text_input("Enter end date (YYYY-MM-DD):")
tweet_count = st.number_input("Enter number of tweets to scrape:")

# Scrape tweets and store in database
if st.button("Scrape Tweets"):
    tweets = scrape_twitter_data(hashtag, start_date, end_date, tweet_count)
    for tweet in tweets:
        collection.insert_one(tweet)
    st.success("Scraped tweets and stored in database!")

if st.checkbox("Show Tweets"):
    tweets = list(collection.find())
    df = pd.DataFrame(tweets)
    st.write(df)

if st.checkbox("Export tweets to CSV"):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="tweets.csv">Download CSV</a>'
    st.markdown(href, unsafe_allow_html=True)
    st.success("Exported tweets to CSV!")

if st.checkbox("Export tweets to JSON"):
    json = df.to_json()
    b64 = base64.b64encode(json.encode()).decode()
    href = f'<a href="data:application/json;base64,{b64}" download="tweets.json">Download JSON</a>'
    st.markdown(href, unsafe_allow_html=True)
    st.success("Exported tweets to JSON!")